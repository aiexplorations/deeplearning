{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Filters in a Simple CNN (for FMNIST data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple classification model built for classifying the FMNIST dataset. The example closely follows the lessons available at Coursera's [Tensorflow Specialization](https://www.deeplearning.ai/tensorflow-in-practice/), taught by Andrew Ng and Lawerence Moroney. We use the tf.keras API to build a `Sequential` model in Keras that uses convolution (`Conv2D`), pooling (`MaxPooling2D`)and fully connected (`Dense`) layers to process FMNIST images and classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We use Tensorflow version 1.x here. Equally, you could use any version of TF and this code should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 9s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing some of the sample images \n",
    "\n",
    "* FMNIST is a collection of fashion centric image data - different kinds of clothing essentially.\n",
    "* These are in grayscale, and have labels associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Image '+str(training_labels[0])+\" \"+str(training_images[0].shape))\n",
    "plt.imshow(training_images[0])\n",
    "plt.show()\n",
    "plt.title('Image '+str(training_labels[2])+\" \"+str(training_images[2].shape))\n",
    "plt.imshow(training_images[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data: Reshaping the Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to reshape the input tensors to be able to feed the 28x28 grayscale (single channel) images to the neural network we're building for classifying the records based on the labels. The original shape (count x depth_pixels x height_pixels) doesn't indicate the number of channels. Therefore, we reshape these tensors into a list of single channel images each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, xts = training_images.reshape(60000,28,28,1), test_images.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a sequential model in tf.keras, with one convolutional layer, with a relu activation, and a pooling layer to compress the convolution output. We then build fully connected layers towards the latter part of the network finally resulting in a 10 class softmax output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of a convolutional network is to detect key features in the image that may be used to differentiate images from one another and associate them with different class labels. Here we use 32 filters in the convolutional layer into which we feed the 60000 training images from the FMNIST dataset. \n",
    "\n",
    "The below model has the following characteristics:\n",
    "1. 32 filters in the Conv2D layer\n",
    "2. A 3x3 size filter is used in each case\n",
    "3. ReLU (Rectified Linear Unit) activation on the convolutional layer\n",
    "4. 2X2 Max Pooling layer to compress the image post-convolution.\n",
    "5. Adam optimizer - we could potentially use SGD, Nesterov, or RMSProp, but we'll just go with Adam here.\n",
    "6. We print the accuracy in the training history for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.3976 - acc: 0.8602\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.2674 - acc: 0.9023\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.2216 - acc: 0.918 - 8s 126us/sample - loss: 0.2215 - acc: 0.9187\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1872 - acc: 0.9316\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1593 - acc: 0.9408\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1363 - acc: 0.9485\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1166 - acc: 0.9568\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0975 - acc: 0.9634\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0821 - acc: 0.9696\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 8s 137us/sample - loss: 0.0714 - acc: 0.9728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x195d1f6be48>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtr, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 74us/sample - loss: 0.3148 - acc: 0.9135\n",
      "0.9135\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(xts, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "If we're targeting the accuracy of the network to be in the ballpark of 99%, we could allow the network to train until then and then stop the training process. We can do this by sending callbacks to the fit() method of the model, and stopping training when a certain loss or accuracy has reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callbackClass(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs={}):\n",
    "            if(logs.get('acc')>0.99):\n",
    "                print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_filter = callbackClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model starts training from the previous saved state of weights. This is because we had executed training earlier, and are now essentially continuing that process for an additional 20 epochs, as long as we don't reach 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0591 - acc: 0.9786\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0523 - acc: 0.9809\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0423 - acc: 0.9846\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0387 - acc: 0.9858\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0349 - acc: 0.9877\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.0304 - acc: 0.9897\n",
      "Epoch 7/20\n",
      "59616/60000 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9903\n",
      "Reached 99% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0269 - acc: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x195c09f82b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtr, training_labels, epochs=20, callbacks=[accuracy_filter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, the training stopped at an accuracy score of 99.03%. The training always stops at the end of an epoch here, given the way the `callbackClass.on_epoch_end()` method has been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with more filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce 128 filters in the first layer, and in doing so, build a new model, `model2`. All other parameters remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 128)       1280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               2769024   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,771,594\n",
      "Trainable params: 2,771,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now call the fit() method on the new model, to experiment with it and see how quickly it gets to 99% accuracy. Training here happens slower in each epoch, because the number of filters has been increased many fold, and there is likely to be a proportionate increase in the amount of time to get through each epoch.\n",
    "\n",
    "We observe that:\n",
    "\n",
    "1. Compared to `model` with 32 filters, `model2` reaches higher accuracy rates in earlier epochs, starting off with a slightly greater accuracy in Epoch 1\n",
    "2. Each epoch, as explained earlier, takes more time to complete because of the increased number of filters.\n",
    "3. At the end of 10 epochs, the original model had 97.28% accuracy, whereas `model2` has reached this milestone in epoch 9, while finishing epoch 10 at 97.97% accuracy. We're getting very close by epoch 10!\n",
    "4. Finally, `model2` was able to reach 99% accuracy 2 full epochs earlier than `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 0.3719 - acc: 0.8662\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 17s 280us/sample - loss: 0.2524 - acc: 0.9082\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 0.2060 - acc: 0.9239\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 17s 291us/sample - loss: 0.1685 - acc: 0.9380\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 18s 303us/sample - loss: 0.1414 - acc: 0.9471\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 19s 320us/sample - loss: 0.1166 - acc: 0.9576\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 20s 329us/sample - loss: 0.0962 - acc: 0.9647\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 20s 339us/sample - loss: 0.0801 - acc: 0.9712\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 21s 348us/sample - loss: 0.0665 - acc: 0.9750\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 21s 348us/sample - loss: 0.0565 - acc: 0.9798\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 23s 377us/sample - loss: 0.0490 - acc: 0.9827\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 22s 365us/sample - loss: 0.0398 - acc: 0.9858\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 22s 372us/sample - loss: 0.0327 - acc: 0.9883\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 22s 373us/sample - loss: 0.0328 - acc: 0.9887\n",
      "Epoch 15/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9912\n",
      "Reached 99% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 23s 385us/sample - loss: 0.0254 - acc: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196b3f1be10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(xtr, training_labels, epochs=20, callbacks=[accuracy_filter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've done a little experiment with scaled MNIST data to see how filters affect model performance. We could do more such experiments with other hyperparameters, such as the following:\n",
    "1. The filter size and pooling size\n",
    "2. Additional convolutional layers\n",
    "3. Strides used in the convolutions\n",
    "\n",
    "Feel free to fork and use the notebook to experiment with these models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
