{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Geological_Image_Similarity_ImageGenerator_and_other_tweaks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiexplorations/deeplearning/blob/master/Geological_Image_Similarity_ImageGenerator_and_other_tweaks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7vZFV9Ump_X",
        "colab_type": "text"
      },
      "source": [
        "# Classifying Geologically Similar Images with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_onlvcHVdhS",
        "colab_type": "text"
      },
      "source": [
        "This notebook explores how the \"Geological similarity\" dataset can be used for a multi-class classification problem. We pull, prepare and build a model to solve the 6-way classification task to different between different kinds of minerals.\n",
        "\n",
        "Specifically, the objective of this notebook is to demonstrate the Image Generator functionality within `tensorflow.keras`, and the benefit of this when dealing with large, clearly labelled data present as image files on disk. We also demonstrate other elements of tweaking deep neural networks, specifically the addition of convolutional and pooling layers.\n",
        "\n",
        "## Data\n",
        "\n",
        "Some notes about the data:\n",
        "1. The images here are 28x28, colour RGB images.\n",
        "2. There are six classes of images: andesite, gneiss, marble, quartzite  rhyolite, and schist\n",
        "\n",
        "## Experiments\n",
        "\n",
        "Some notes about the model and the experiments:\n",
        "1. Tried building a simple CNN with a single `Conv2D` layer, which didn't perform as well.\n",
        "2. Subsequent iterations increased the number of `Conv2D` layers, married to MaxPooling2D layers\n",
        "3. The number of epochs required to train simpler models to reach ~99% accuracy was high, of the order of 100.\n",
        "4. When more complex models were used, these issues were resolved, with 99% accuracy being reached in 60-odd epochs.\n",
        "\n",
        "\n",
        "## Techniques Demonstrated\n",
        "\n",
        "1. Image Generator - a built-in method within Keras that accelerates the process of creating labelled data by just pointing to a file system folder with the label names being folder names. A very handy tool.\n",
        "2. Early stopping using accuracy callbacks, which enables easier retraining of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcdFk8sGGjkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWZMsuORblcI",
        "colab_type": "text"
      },
      "source": [
        "We pull in the geological similarity data in the below cell, and in subsequent cells, store the data in a local folder in the environment/container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QQZyU4GGjkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_url = \"http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "outputId": "f786890c-5cc8-42ab-9784-e526382e508a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip \\\n",
        "    -O /tmp/geological_similarity.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-08 08:55:37--  http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\n",
            "Resolving aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)... 52.218.176.227\n",
            "Connecting to aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)|52.218.176.227|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35312590 (34M) [application/zip]\n",
            "Saving to: ‘/tmp/geological_similarity.zip’\n",
            "\n",
            "/tmp/geological_sim 100%[===================>]  33.68M  46.1MB/s    in 0.7s    \n",
            "\n",
            "2020-06-08 08:55:38 (46.1 MB/s) - ‘/tmp/geological_similarity.zip’ saved [35312590/35312590]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/geological_similarity.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCyQ3uLOHLw5",
        "colab_type": "code",
        "outputId": "b0d2370b-7ed4-4ae8-9c29-48589e2fe5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! ls /tmp/geological_similarity/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "andesite  gneiss  marble  quartzite  rhyolite  schist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvWIjCH-buxM",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Model\n",
        "\n",
        "Here, we've defined a `Sequential` model in Keras of relatively high complexity, compared to the simple models we see for the FMNIST data classification task. There are three `Conv2D` layers, with associated pooling layers. The third set of layers uses a smaller filter size compared the earlier ones.\n",
        "\n",
        "The flattened results are then taken to a DNN, which then outputs to a `Softmax` layer to do the multi-class classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2J9jgiJHuug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bK0uqxWH4lB",
        "colab_type": "code",
        "outputId": "3cda74b3-3686-4844-f774-4268d57cb304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 256)       7168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 128)       295040    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 64)          32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 343,462\n",
            "Trainable params: 343,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aURY4pqXIJca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "'''\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='RMSprop(lr=0.001)',\n",
        "              metrics=['accuracy'])\n",
        "'''\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpAFKLhLcMyb",
        "colab_type": "text"
      },
      "source": [
        "## Using Image Data Generators\n",
        "\n",
        "The image data generator class from Keras' preprocessing module has been used to scale and build the `train_generator` instance. This instance of the class takes in the images in the `geological_similarity` folder, and then prepares the training data for *sparse categorical crossentropy* loss. This means that the output will be a tensor where the number of columns in the tensor will be equal to the number of classes in the classification problem statement.\n",
        "\n",
        "The image data generator makes short work of the potentially laborious task of labelling thousands of images, as long as the images are present in different folders.\n",
        "\n",
        "## Training and Test Sets\n",
        "When specifying the data generator, we can set up a `validation_split` ratio within the constructor, which enables the generator to be pointed to the same location for generating distinct training and test sets.\n",
        "\n",
        "## Image augmentation techniques\n",
        "\n",
        "By using shearing, scaling, rotation and such geometric operations on the images, we can potentially obtain richer data for the training process, which will enable faster convergence, or better performance. Below we will define such image augmentation methods using the `ImageDataGenerator` class's built-in arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lf-chLLISYP",
        "colab_type": "code",
        "outputId": "9f8e46c7-5721-4f0b-dc71-12364926a75f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "image_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                   validation_split = 0.25, # 25% of the data kept for validation\n",
        "                                   height_shift_range = 0.2, #the image is shifted up to 20% in the vertical direction, randomly\n",
        "                                   width_shift_range = 0.2, # the image is shifted up to 20% in the horizontal direction randomly\n",
        "                                   shear_range = 0.2, # the image is sheared up to 20% of the width randomly\n",
        "                                   horizontal_flip = True, #flip images horizontally\n",
        "                                   vertical_flip = True, #flip images vertically  \n",
        "                                   )\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = image_datagen.flow_from_directory(\n",
        "        '/tmp/geological_similarity',  # This is the source directory for training images\n",
        "        target_size=(28, 28),  # All images will be resized to 28x28\n",
        "        batch_size=128,\n",
        "        # Since we use sparse_categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse',\n",
        "        subset = \"training\")\n",
        "\n",
        "\n",
        "test_generator = image_datagen.flow_from_directory(\n",
        "        '/tmp/geological_similarity',  # This is the source directory for training images\n",
        "        target_size=(28, 28),  # All images will be resized to 28x28\n",
        "        batch_size=128,\n",
        "        # Since we use sparse_categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse',\n",
        "        subset = \"validation\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22499 images belonging to 6 classes.\n",
            "Found 7499 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFKxHdKqczMe",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Early stopping\n",
        "\n",
        "The `callbackClass()` class here enables us to stop training the neural network when we reach a certain level of accuracy. In this case, we're looking for 99% accuracy on the training data. We don't have a test dataset here, potentially that could be added as well, if required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_n7AyJcJvls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class callbackClass(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('accuracy')>0.99):\n",
        "                print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "accuracy_filter = callbackClass()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaCXef5dBs4",
        "colab_type": "text"
      },
      "source": [
        "In the cell below, we train the model over 100 maximum epochs, with the accuracy filter enabling us to stop early if the required accuracy has been reached. We can specify additional parameters such as the batch size, and if we possess test data, we could use that too to get validation statistics.\n",
        "\n",
        "## Using Validation Data in Model Training\n",
        "\n",
        "In the model.fit() method, we can introduce the validation_data argument and supply `test_generator` to it. Keras has made this process of supplying validation data really simple!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdLv25-0IhcL",
        "colab_type": "code",
        "outputId": "1b24c4ee-3243-453b-e63a-001003a552e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      batch_size=128,  \n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      callbacks = [accuracy_filter])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0426 - accuracy: 0.9845 - val_loss: 0.0430 - val_accuracy: 0.9856\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0435 - accuracy: 0.9845 - val_loss: 0.0525 - val_accuracy: 0.9820\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 26s 146ms/step - loss: 0.0451 - accuracy: 0.9837 - val_loss: 0.0386 - val_accuracy: 0.9868\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0505 - accuracy: 0.9820 - val_loss: 0.0487 - val_accuracy: 0.9827\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0461 - accuracy: 0.9839 - val_loss: 0.0413 - val_accuracy: 0.9861\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0418 - accuracy: 0.9848 - val_loss: 0.0663 - val_accuracy: 0.9760\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0384 - accuracy: 0.9864 - val_loss: 0.0546 - val_accuracy: 0.9811\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0435 - accuracy: 0.9842 - val_loss: 0.0429 - val_accuracy: 0.9867\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 26s 149ms/step - loss: 0.0422 - accuracy: 0.9845 - val_loss: 0.0539 - val_accuracy: 0.9815\n",
            "Epoch 10/100\n",
            "176/176 [==============================] - 26s 150ms/step - loss: 0.0422 - accuracy: 0.9842 - val_loss: 0.0533 - val_accuracy: 0.9817\n",
            "Epoch 11/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0418 - accuracy: 0.9853 - val_loss: 0.0513 - val_accuracy: 0.9823\n",
            "Epoch 12/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0479 - accuracy: 0.9825 - val_loss: 0.0736 - val_accuracy: 0.9735\n",
            "Epoch 13/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0465 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9859\n",
            "Epoch 14/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0376 - accuracy: 0.9857 - val_loss: 0.0387 - val_accuracy: 0.9873\n",
            "Epoch 15/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0416 - accuracy: 0.9849 - val_loss: 0.0616 - val_accuracy: 0.9791\n",
            "Epoch 16/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0407 - accuracy: 0.9854 - val_loss: 0.0444 - val_accuracy: 0.9847\n",
            "Epoch 17/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0398 - accuracy: 0.9853 - val_loss: 0.0403 - val_accuracy: 0.9873\n",
            "Epoch 18/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0429 - accuracy: 0.9846 - val_loss: 0.1159 - val_accuracy: 0.9589\n",
            "Epoch 19/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0367 - accuracy: 0.9863 - val_loss: 0.0406 - val_accuracy: 0.9868\n",
            "Epoch 20/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0381 - accuracy: 0.9853 - val_loss: 0.0439 - val_accuracy: 0.9844\n",
            "Epoch 21/100\n",
            "176/176 [==============================] - 24s 139ms/step - loss: 0.0406 - accuracy: 0.9847 - val_loss: 0.0417 - val_accuracy: 0.9857\n",
            "Epoch 22/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0395 - accuracy: 0.9860 - val_loss: 0.0447 - val_accuracy: 0.9840\n",
            "Epoch 23/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0387 - accuracy: 0.9865 - val_loss: 0.0357 - val_accuracy: 0.9880\n",
            "Epoch 24/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0438 - accuracy: 0.9848 - val_loss: 0.0517 - val_accuracy: 0.9800\n",
            "Epoch 25/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0395 - accuracy: 0.9851 - val_loss: 0.0523 - val_accuracy: 0.9801\n",
            "Epoch 26/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0415 - accuracy: 0.9852 - val_loss: 0.0487 - val_accuracy: 0.9843\n",
            "Epoch 27/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0414 - accuracy: 0.9859 - val_loss: 0.0598 - val_accuracy: 0.9791\n",
            "Epoch 28/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0361 - accuracy: 0.9862 - val_loss: 0.0416 - val_accuracy: 0.9853\n",
            "Epoch 29/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0394 - accuracy: 0.9856 - val_loss: 0.0409 - val_accuracy: 0.9871\n",
            "Epoch 30/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0349 - accuracy: 0.9871 - val_loss: 0.0462 - val_accuracy: 0.9836\n",
            "Epoch 31/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0394 - accuracy: 0.9855 - val_loss: 0.0403 - val_accuracy: 0.9864\n",
            "Epoch 32/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0423 - accuracy: 0.9841 - val_loss: 0.0427 - val_accuracy: 0.9869\n",
            "Epoch 33/100\n",
            "176/176 [==============================] - 25s 145ms/step - loss: 0.0402 - accuracy: 0.9855 - val_loss: 0.0534 - val_accuracy: 0.9809\n",
            "Epoch 34/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0380 - accuracy: 0.9868 - val_loss: 0.0569 - val_accuracy: 0.9791\n",
            "Epoch 35/100\n",
            "176/176 [==============================] - 26s 147ms/step - loss: 0.0376 - accuracy: 0.9865 - val_loss: 0.0389 - val_accuracy: 0.9876\n",
            "Epoch 36/100\n",
            "176/176 [==============================] - 26s 148ms/step - loss: 0.0383 - accuracy: 0.9851 - val_loss: 0.0660 - val_accuracy: 0.9759\n",
            "Epoch 37/100\n",
            "176/176 [==============================] - 26s 146ms/step - loss: 0.0368 - accuracy: 0.9872 - val_loss: 0.0367 - val_accuracy: 0.9875\n",
            "Epoch 38/100\n",
            "176/176 [==============================] - 25s 145ms/step - loss: 0.0341 - accuracy: 0.9875 - val_loss: 0.0460 - val_accuracy: 0.9821\n",
            "Epoch 39/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0395 - accuracy: 0.9858 - val_loss: 0.0427 - val_accuracy: 0.9864\n",
            "Epoch 40/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0350 - accuracy: 0.9871 - val_loss: 0.0466 - val_accuracy: 0.9851\n",
            "Epoch 41/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0378 - accuracy: 0.9865 - val_loss: 0.0545 - val_accuracy: 0.9807\n",
            "Epoch 42/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0377 - accuracy: 0.9867 - val_loss: 0.0445 - val_accuracy: 0.9840\n",
            "Epoch 43/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0384 - accuracy: 0.9863 - val_loss: 0.0407 - val_accuracy: 0.9863\n",
            "Epoch 44/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0434 - accuracy: 0.9835 - val_loss: 0.0499 - val_accuracy: 0.9816\n",
            "Epoch 45/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0357 - accuracy: 0.9867 - val_loss: 0.0429 - val_accuracy: 0.9853\n",
            "Epoch 46/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0317 - accuracy: 0.9879 - val_loss: 0.0387 - val_accuracy: 0.9867\n",
            "Epoch 47/100\n",
            "176/176 [==============================] - 26s 148ms/step - loss: 0.0376 - accuracy: 0.9857 - val_loss: 0.0433 - val_accuracy: 0.9849\n",
            "Epoch 48/100\n",
            "176/176 [==============================] - 26s 149ms/step - loss: 0.0357 - accuracy: 0.9871 - val_loss: 0.0369 - val_accuracy: 0.9877\n",
            "Epoch 49/100\n",
            "176/176 [==============================] - 26s 149ms/step - loss: 0.0318 - accuracy: 0.9883 - val_loss: 0.0432 - val_accuracy: 0.9857\n",
            "Epoch 50/100\n",
            "176/176 [==============================] - 26s 149ms/step - loss: 0.0420 - accuracy: 0.9852 - val_loss: 0.0343 - val_accuracy: 0.9881\n",
            "Epoch 51/100\n",
            "176/176 [==============================] - 26s 149ms/step - loss: 0.0329 - accuracy: 0.9879 - val_loss: 0.0522 - val_accuracy: 0.9791\n",
            "Epoch 52/100\n",
            "176/176 [==============================] - 25s 145ms/step - loss: 0.0400 - accuracy: 0.9847 - val_loss: 0.0458 - val_accuracy: 0.9833\n",
            "Epoch 53/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0334 - accuracy: 0.9874 - val_loss: 0.0357 - val_accuracy: 0.9896\n",
            "Epoch 54/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0328 - accuracy: 0.9878 - val_loss: 0.0420 - val_accuracy: 0.9865\n",
            "Epoch 55/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 0.0381 - val_accuracy: 0.9879\n",
            "Epoch 56/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0369 - accuracy: 0.9867 - val_loss: 0.0438 - val_accuracy: 0.9849\n",
            "Epoch 57/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0310 - accuracy: 0.9896 - val_loss: 0.0672 - val_accuracy: 0.9755\n",
            "Epoch 58/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0332 - accuracy: 0.9878 - val_loss: 0.0435 - val_accuracy: 0.9853\n",
            "Epoch 59/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0349 - accuracy: 0.9870 - val_loss: 0.0515 - val_accuracy: 0.9816\n",
            "Epoch 60/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0332 - accuracy: 0.9883 - val_loss: 0.0628 - val_accuracy: 0.9783\n",
            "Epoch 61/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0323 - accuracy: 0.9882 - val_loss: 0.0408 - val_accuracy: 0.9844\n",
            "Epoch 62/100\n",
            "176/176 [==============================] - 26s 147ms/step - loss: 0.0355 - accuracy: 0.9873 - val_loss: 0.0438 - val_accuracy: 0.9849\n",
            "Epoch 63/100\n",
            "176/176 [==============================] - 26s 148ms/step - loss: 0.0329 - accuracy: 0.9877 - val_loss: 0.0370 - val_accuracy: 0.9864\n",
            "Epoch 64/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0359 - accuracy: 0.9867 - val_loss: 0.0429 - val_accuracy: 0.9849\n",
            "Epoch 65/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0363 - accuracy: 0.9868 - val_loss: 0.0449 - val_accuracy: 0.9864\n",
            "Epoch 66/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0353 - accuracy: 0.9875 - val_loss: 0.0323 - val_accuracy: 0.9900\n",
            "Epoch 67/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0338 - accuracy: 0.9874 - val_loss: 0.0525 - val_accuracy: 0.9821\n",
            "Epoch 68/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 0.0387 - val_accuracy: 0.9868\n",
            "Epoch 69/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0357 - accuracy: 0.9872 - val_loss: 0.0389 - val_accuracy: 0.9868\n",
            "Epoch 70/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0294 - accuracy: 0.9900 - val_loss: 0.0339 - val_accuracy: 0.9881\n",
            "Epoch 71/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0302 - accuracy: 0.9888 - val_loss: 0.0428 - val_accuracy: 0.9845\n",
            "Epoch 72/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0312 - accuracy: 0.9887 - val_loss: 0.0383 - val_accuracy: 0.9863\n",
            "Epoch 73/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0351 - accuracy: 0.9870 - val_loss: 0.0367 - val_accuracy: 0.9879\n",
            "Epoch 74/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0332 - accuracy: 0.9884 - val_loss: 0.0362 - val_accuracy: 0.9869\n",
            "Epoch 75/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0294 - accuracy: 0.9892 - val_loss: 0.0659 - val_accuracy: 0.9760\n",
            "Epoch 76/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0332 - accuracy: 0.9875 - val_loss: 0.0366 - val_accuracy: 0.9879\n",
            "Epoch 77/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0298 - accuracy: 0.9896 - val_loss: 0.0354 - val_accuracy: 0.9884\n",
            "Epoch 78/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0306 - accuracy: 0.9886 - val_loss: 0.0352 - val_accuracy: 0.9892\n",
            "Epoch 79/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0321 - accuracy: 0.9884 - val_loss: 0.0353 - val_accuracy: 0.9865\n",
            "Epoch 80/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0320 - accuracy: 0.9879 - val_loss: 0.0492 - val_accuracy: 0.9821\n",
            "Epoch 81/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0303 - accuracy: 0.9891 - val_loss: 0.0557 - val_accuracy: 0.9817\n",
            "Epoch 82/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0345 - accuracy: 0.9875 - val_loss: 0.0359 - val_accuracy: 0.9889\n",
            "Epoch 83/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0330 - accuracy: 0.9883 - val_loss: 0.0527 - val_accuracy: 0.9833\n",
            "Epoch 84/100\n",
            "176/176 [==============================] - 25s 144ms/step - loss: 0.0357 - accuracy: 0.9868 - val_loss: 0.0354 - val_accuracy: 0.9883\n",
            "Epoch 85/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0316 - accuracy: 0.9884 - val_loss: 0.0327 - val_accuracy: 0.9884\n",
            "Epoch 86/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0291 - accuracy: 0.9896 - val_loss: 0.0437 - val_accuracy: 0.9847\n",
            "Epoch 87/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0306 - accuracy: 0.9884 - val_loss: 0.0397 - val_accuracy: 0.9867\n",
            "Epoch 88/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0300 - accuracy: 0.9896 - val_loss: 0.0429 - val_accuracy: 0.9848\n",
            "Epoch 89/100\n",
            "176/176 [==============================] - 25s 143ms/step - loss: 0.0391 - accuracy: 0.9872 - val_loss: 0.0390 - val_accuracy: 0.9863\n",
            "Epoch 90/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0305 - accuracy: 0.9891 - val_loss: 0.0384 - val_accuracy: 0.9871\n",
            "Epoch 91/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0322 - accuracy: 0.9882 - val_loss: 0.0321 - val_accuracy: 0.9880\n",
            "Epoch 92/100\n",
            "176/176 [==============================] - 25s 141ms/step - loss: 0.0303 - accuracy: 0.9896 - val_loss: 0.0374 - val_accuracy: 0.9884\n",
            "Epoch 93/100\n",
            "176/176 [==============================] - 25s 142ms/step - loss: 0.0289 - accuracy: 0.9893 - val_loss: 0.0337 - val_accuracy: 0.9899\n",
            "Epoch 94/100\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0292 - accuracy: 0.9893 - val_loss: 0.0407 - val_accuracy: 0.9860\n",
            "Epoch 95/100\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9912\n",
            "Reached 99% accuracy so cancelling training!\n",
            "176/176 [==============================] - 25s 140ms/step - loss: 0.0251 - accuracy: 0.9912 - val_loss: 0.0352 - val_accuracy: 0.9873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFXPzrW2VUK7",
        "colab_type": "text"
      },
      "source": [
        "We see that 99% accuracy has been reached, and the training process has been stopped as a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKknwV5IIkRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}