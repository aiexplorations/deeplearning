{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Geological_Image_Similarity_ImageGenerator_and_other_tweaks",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_onlvcHVdhS",
        "colab_type": "text"
      },
      "source": [
        "This notebook explores how the \"Geological similarity\" dataset can be used for a multi-class classification problem. We pull, prepare and build a model to solve the 6-way classification task to different between different kinds of minerals.\n",
        "\n",
        "Specifically, the objective of this notebook is to demonstrate the Image Generator functionality within `tensorflow.keras`, and the benefit of this when dealing with large, clearly labelled data present as image files on disk. We also demonstrate other elements of tweaking deep neural networks, specifically the addition of convolutional and pooling layers.\n",
        "\n",
        "Some notes about the data:\n",
        "1. The images here are 28x28, colour RGB images.\n",
        "2. There are six classes of images: andesite, gneiss, marble, quartzite  rhyolite, and schist\n",
        "\n",
        "Some notes about the model and the experiments:\n",
        "1. Tried building a simple CNN with a single `Conv2D` layer, which didn't perform as well.\n",
        "2. Subsequent iterations increased the number of `Conv2D` layers, married to MaxPooling2D layers\n",
        "3. The number of epochs required to train simpler models to reach ~99% accuracy was high, of the order of 100.\n",
        "4. When more complex models were used, these issues were resolved, with 99% accuracy being reached in 60-odd epochs.\n",
        "\n",
        "Other techniques used here:\n",
        "1. Image Generator - a built-in method within Keras that accelerates the process of creating labelled data by just pointing to a file system folder with the label names being folder names. A very handy tool.\n",
        "2. Early stopping using accuracy callbacks, which enables easier retraining of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcdFk8sGGjkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWZMsuORblcI",
        "colab_type": "text"
      },
      "source": [
        "We pull in the geological similarity data in the below cell, and in subsequent cells, store the data in a local folder in the environment/container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QQZyU4GGjkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_url = \"http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "6a21d4f5-6194-409a-d2da-9267bc5d21da"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip \\\n",
        "    -O /tmp/geological_similarity.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-31 10:36:33--  http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\n",
            "Resolving aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)... 52.218.224.66\n",
            "Connecting to aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)|52.218.224.66|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35312590 (34M) [application/zip]\n",
            "Saving to: ‘/tmp/geological_similarity.zip’\n",
            "\n",
            "\r          /tmp/geol   0%[                    ]       0  --.-KB/s               \r         /tmp/geolo  47%[========>           ]  16.00M  65.6MB/s               \r        /tmp/geolog  71%[=============>      ]  24.00M  52.9MB/s               \r/tmp/geological_sim 100%[===================>]  33.68M  56.8MB/s    in 0.6s    \n",
            "\n",
            "2020-05-31 10:36:34 (56.8 MB/s) - ‘/tmp/geological_similarity.zip’ saved [35312590/35312590]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/geological_similarity.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCyQ3uLOHLw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "02fb5088-ba8b-4e9a-80c2-6a12be96a67b"
      },
      "source": [
        "! ls /tmp/geological_similarity/geological_similarity"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "andesite  gneiss  marble  quartzite  rhyolite  schist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvWIjCH-buxM",
        "colab_type": "text"
      },
      "source": [
        "Here, we've defined a `Sequential` model in Keras of relatively high complexity, compared to the simple models we see for the FMNIST data classification task. There are three `Conv2D` layers, with associated pooling layers. The third set of layers uses a smaller filter size compared the earlier ones.\n",
        "\n",
        "The flattened results are then taken to a DNN, which then outputs to a `Softmax` layer to do the multi-class classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2J9jgiJHuug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bK0uqxWH4lB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "f7c3df61-8b0d-49f0-c158-119affcf5a8c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 256)       7168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 128)       295040    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 64)          32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 343,462\n",
            "Trainable params: 343,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aURY4pqXIJca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "'''\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='RMSprop(lr=0.001)',\n",
        "              metrics=['accuracy'])\n",
        "'''\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpAFKLhLcMyb",
        "colab_type": "text"
      },
      "source": [
        "The image data generator class from Keras' preprocessing module has been used to scale and build the `train_generator` instance. This instance of the class takes in the images in the `geological_similarity` folder, and then prepares the training data for *sparse categorical crossentropy* loss. This means that the output will be a tensor where the number of columns in the tensor will be equal to the number of classes in the classification problem statement.\n",
        "\n",
        "The image data generator makes short work of the potentially laborious task of labelling thousands of images, as long as the images are present in different folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lf-chLLISYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1ae0509-f353-482a-e51f-09e61c153768"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/geological_similarity/geological_similarity',  # This is the source directory for training images\n",
        "        target_size=(28, 28),  # All images will be resized to 28x28\n",
        "        batch_size=128,\n",
        "        # Since we use sparse_categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 29998 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFKxHdKqczMe",
        "colab_type": "text"
      },
      "source": [
        "The `callbackClass()` class here enables us to stop training the neural network when we reach a certain level of accuracy. In this case, we're looking for 99% accuracy on the training data. We don't have a test dataset here, potentially that could be added as well, if required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_n7AyJcJvls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class callbackClass(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('accuracy')>0.99):\n",
        "                print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "accuracy_filter = callbackClass()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaCXef5dBs4",
        "colab_type": "text"
      },
      "source": [
        "In the cell below, we train the model over 100 maximum epochs, with the accuracy filter enabling us to stop early if the required accuracy has been reached. We can specify additional parameters such as the batch size, and if we possess test data, we could use that too to get validation statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdLv25-0IhcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b1c36ef-b15d-4547-e3f6-01c2cc31548f"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      batch_size=16,  \n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      callbacks = [accuracy_filter])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.5807 - accuracy: 0.7609\n",
            "Epoch 2/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.2822 - accuracy: 0.8920\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.2236 - accuracy: 0.9154\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1883 - accuracy: 0.9312\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.1808 - accuracy: 0.9339\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1704 - accuracy: 0.9377\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1561 - accuracy: 0.9426\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1385 - accuracy: 0.9500\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.1307 - accuracy: 0.9543\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1382 - accuracy: 0.9504\n",
            "Epoch 11/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1252 - accuracy: 0.9556\n",
            "Epoch 12/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1289 - accuracy: 0.9523\n",
            "Epoch 13/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1153 - accuracy: 0.9584\n",
            "Epoch 14/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1149 - accuracy: 0.9584\n",
            "Epoch 15/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1066 - accuracy: 0.9620\n",
            "Epoch 16/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0987 - accuracy: 0.9650\n",
            "Epoch 17/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1002 - accuracy: 0.9649\n",
            "Epoch 18/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1022 - accuracy: 0.9630\n",
            "Epoch 19/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0962 - accuracy: 0.9661\n",
            "Epoch 20/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0931 - accuracy: 0.9674\n",
            "Epoch 21/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0834 - accuracy: 0.9707\n",
            "Epoch 22/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.1017 - accuracy: 0.9630\n",
            "Epoch 23/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0933 - accuracy: 0.9673\n",
            "Epoch 24/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0802 - accuracy: 0.9716\n",
            "Epoch 25/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0821 - accuracy: 0.9709\n",
            "Epoch 26/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0803 - accuracy: 0.9716\n",
            "Epoch 27/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0741 - accuracy: 0.9739\n",
            "Epoch 28/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0794 - accuracy: 0.9728\n",
            "Epoch 29/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0661 - accuracy: 0.9768\n",
            "Epoch 30/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0719 - accuracy: 0.9748\n",
            "Epoch 31/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0684 - accuracy: 0.9759\n",
            "Epoch 32/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0642 - accuracy: 0.9778\n",
            "Epoch 33/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0716 - accuracy: 0.9745\n",
            "Epoch 34/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0696 - accuracy: 0.9746\n",
            "Epoch 35/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0591 - accuracy: 0.9790\n",
            "Epoch 36/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0575 - accuracy: 0.9789\n",
            "Epoch 37/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0601 - accuracy: 0.9783\n",
            "Epoch 38/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.0520 - accuracy: 0.9813\n",
            "Epoch 39/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0515 - accuracy: 0.9820\n",
            "Epoch 40/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0559 - accuracy: 0.9791\n",
            "Epoch 41/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.0520 - accuracy: 0.9811\n",
            "Epoch 42/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0493 - accuracy: 0.9823\n",
            "Epoch 43/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0549 - accuracy: 0.9805\n",
            "Epoch 44/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0536 - accuracy: 0.9810\n",
            "Epoch 45/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0454 - accuracy: 0.9832\n",
            "Epoch 46/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0451 - accuracy: 0.9839\n",
            "Epoch 47/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0498 - accuracy: 0.9819\n",
            "Epoch 48/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0412 - accuracy: 0.9850\n",
            "Epoch 49/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0408 - accuracy: 0.9845\n",
            "Epoch 50/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0392 - accuracy: 0.9856\n",
            "Epoch 51/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0434 - accuracy: 0.9846\n",
            "Epoch 52/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0454 - accuracy: 0.9827\n",
            "Epoch 53/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0412 - accuracy: 0.9857\n",
            "Epoch 54/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0312 - accuracy: 0.9888\n",
            "Epoch 55/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0338 - accuracy: 0.9888\n",
            "Epoch 56/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0398 - accuracy: 0.9856\n",
            "Epoch 57/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0309 - accuracy: 0.9892\n",
            "Epoch 58/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0383 - accuracy: 0.9859\n",
            "Epoch 59/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0348 - accuracy: 0.9875\n",
            "Epoch 60/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0363 - accuracy: 0.9864\n",
            "Epoch 61/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0335 - accuracy: 0.9877\n",
            "Epoch 62/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0338 - accuracy: 0.9883\n",
            "Epoch 63/100\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9905\n",
            "Reached 99% accuracy so cancelling training!\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.0280 - accuracy: 0.9905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFXPzrW2VUK7",
        "colab_type": "text"
      },
      "source": [
        "We see that 99% accuracy has been reached, and the training process has been stopped as a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKknwV5IIkRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}