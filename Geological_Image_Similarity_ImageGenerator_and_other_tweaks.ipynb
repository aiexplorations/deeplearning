{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Geological_Image_Similarity_ImageGenerator_and_other_tweaks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiexplorations/deeplearning/blob/master/Geological_Image_Similarity_ImageGenerator_and_other_tweaks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7vZFV9Ump_X",
        "colab_type": "text"
      },
      "source": [
        "# Classifying Geologically Similar Images with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_onlvcHVdhS",
        "colab_type": "text"
      },
      "source": [
        "This notebook explores how the \"Geological similarity\" dataset can be used for a multi-class classification problem. We pull, prepare and build a model to solve the 6-way classification task to different between different kinds of minerals.\n",
        "\n",
        "Specifically, the objective of this notebook is to demonstrate the Image Generator functionality within `tensorflow.keras`, and the benefit of this when dealing with large, clearly labelled data present as image files on disk. We also demonstrate other elements of tweaking deep neural networks, specifically the addition of convolutional and pooling layers.\n",
        "\n",
        "## Data\n",
        "\n",
        "Some notes about the data:\n",
        "1. The images here are 28x28, colour RGB images.\n",
        "2. There are six classes of images: andesite, gneiss, marble, quartzite  rhyolite, and schist\n",
        "\n",
        "## Experiments\n",
        "\n",
        "Some notes about the model and the experiments:\n",
        "1. Tried building a simple CNN with a single `Conv2D` layer, which didn't perform as well.\n",
        "2. Subsequent iterations increased the number of `Conv2D` layers, married to MaxPooling2D layers\n",
        "3. The number of epochs required to train simpler models to reach ~99% accuracy was high, of the order of 100.\n",
        "4. When more complex models were used, these issues were resolved, with 99% accuracy being reached in 60-odd epochs.\n",
        "\n",
        "\n",
        "## Techniques Demonstrated\n",
        "\n",
        "1. Image Generator - a built-in method within Keras that accelerates the process of creating labelled data by just pointing to a file system folder with the label names being folder names. A very handy tool.\n",
        "2. Early stopping using accuracy callbacks, which enables easier retraining of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcdFk8sGGjkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWZMsuORblcI",
        "colab_type": "text"
      },
      "source": [
        "We pull in the geological similarity data in the below cell, and in subsequent cells, store the data in a local folder in the environment/container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QQZyU4GGjkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_url = \"http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "outputId": "9cc10c99-afe9-4245-b303-ffaea8b80741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip \\\n",
        "    -O /tmp/geological_similarity.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-02 11:46:47--  http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\n",
            "Resolving aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)... 52.218.248.58\n",
            "Connecting to aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)|52.218.248.58|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35312590 (34M) [application/zip]\n",
            "Saving to: ‘/tmp/geological_similarity.zip’\n",
            "\n",
            "/tmp/geological_sim 100%[===================>]  33.68M  28.2MB/s    in 1.2s    \n",
            "\n",
            "2020-06-02 11:46:48 (28.2 MB/s) - ‘/tmp/geological_similarity.zip’ saved [35312590/35312590]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/geological_similarity.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCyQ3uLOHLw5",
        "colab_type": "code",
        "outputId": "9b54531a-99b7-462f-bff3-90345ffeb5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! ls /tmp/geological_similarity/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "andesite  gneiss  marble  quartzite  rhyolite  schist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvWIjCH-buxM",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Model\n",
        "\n",
        "Here, we've defined a `Sequential` model in Keras of relatively high complexity, compared to the simple models we see for the FMNIST data classification task. There are three `Conv2D` layers, with associated pooling layers. The third set of layers uses a smaller filter size compared the earlier ones.\n",
        "\n",
        "The flattened results are then taken to a DNN, which then outputs to a `Softmax` layer to do the multi-class classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2J9jgiJHuug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bK0uqxWH4lB",
        "colab_type": "code",
        "outputId": "a1ea11f1-774c-4f6d-e8b6-9aecf54b0263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 256)       7168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 128)       295040    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 64)          32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 343,462\n",
            "Trainable params: 343,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aURY4pqXIJca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "'''\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='RMSprop(lr=0.001)',\n",
        "              metrics=['accuracy'])\n",
        "'''\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpAFKLhLcMyb",
        "colab_type": "text"
      },
      "source": [
        "## Using Image Data Generators\n",
        "\n",
        "The image data generator class from Keras' preprocessing module has been used to scale and build the `train_generator` instance. This instance of the class takes in the images in the `geological_similarity` folder, and then prepares the training data for *sparse categorical crossentropy* loss. This means that the output will be a tensor where the number of columns in the tensor will be equal to the number of classes in the classification problem statement.\n",
        "\n",
        "The image data generator makes short work of the potentially laborious task of labelling thousands of images, as long as the images are present in different folders.\n",
        "\n",
        "## Training and Test Sets\n",
        "When specifying the data generator, we can set up a `validation_split` ratio within the constructor, which enables the generator to be pointed to the same location for generating distinct training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lf-chLLISYP",
        "colab_type": "code",
        "outputId": "aa35353b-278e-4eb8-81d4-371aa2dba9a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "image_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                   validation_split = 0.25)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = image_datagen.flow_from_directory(\n",
        "        '/tmp/geological_similarity',  # This is the source directory for training images\n",
        "        target_size=(28, 28),  # All images will be resized to 28x28\n",
        "        batch_size=128,\n",
        "        # Since we use sparse_categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse',\n",
        "        subset = \"training\")\n",
        "\n",
        "\n",
        "test_generator = image_datagen.flow_from_directory(\n",
        "        '/tmp/geological_similarity',  # This is the source directory for training images\n",
        "        target_size=(28, 28),  # All images will be resized to 28x28\n",
        "        batch_size=128,\n",
        "        # Since we use sparse_categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse',\n",
        "        subset = \"validation\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22499 images belonging to 6 classes.\n",
            "Found 7499 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFKxHdKqczMe",
        "colab_type": "text"
      },
      "source": [
        "The `callbackClass()` class here enables us to stop training the neural network when we reach a certain level of accuracy. In this case, we're looking for 99% accuracy on the training data. We don't have a test dataset here, potentially that could be added as well, if required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_n7AyJcJvls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class callbackClass(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('accuracy')>0.99):\n",
        "                print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "accuracy_filter = callbackClass()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaCXef5dBs4",
        "colab_type": "text"
      },
      "source": [
        "In the cell below, we train the model over 100 maximum epochs, with the accuracy filter enabling us to stop early if the required accuracy has been reached. We can specify additional parameters such as the batch size, and if we possess test data, we could use that too to get validation statistics.\n",
        "\n",
        "## Using Validation Data in Model Training\n",
        "\n",
        "In the model.fit() method, we can introduce the validation_data argument and supply `test_generator` to it. Keras has made this process of supplying validation data really simple!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdLv25-0IhcL",
        "colab_type": "code",
        "outputId": "69d3f22f-e916-4568-d4c6-751c93414854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      batch_size=16,  \n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      callbacks = [accuracy_filter])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "176/176 [==============================] - 13s 73ms/step - loss: 0.6190 - accuracy: 0.7479 - val_loss: 0.3657 - val_accuracy: 0.8558\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.3024 - accuracy: 0.8824 - val_loss: 0.2425 - val_accuracy: 0.9085\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 13s 74ms/step - loss: 0.2478 - accuracy: 0.9064 - val_loss: 0.2037 - val_accuracy: 0.9283\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.2076 - accuracy: 0.9234 - val_loss: 0.2484 - val_accuracy: 0.9091\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.1867 - accuracy: 0.9341 - val_loss: 0.1560 - val_accuracy: 0.9460\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.1735 - accuracy: 0.9376 - val_loss: 0.1685 - val_accuracy: 0.9375\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.1820 - accuracy: 0.9324 - val_loss: 0.1531 - val_accuracy: 0.9484\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.1649 - accuracy: 0.9411 - val_loss: 0.1773 - val_accuracy: 0.9377\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.1538 - accuracy: 0.9465 - val_loss: 0.1331 - val_accuracy: 0.9517\n",
            "Epoch 10/100\n",
            "176/176 [==============================] - 13s 71ms/step - loss: 0.1435 - accuracy: 0.9483 - val_loss: 0.1303 - val_accuracy: 0.9563\n",
            "Epoch 11/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.1438 - accuracy: 0.9480 - val_loss: 0.1272 - val_accuracy: 0.9557\n",
            "Epoch 12/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.1257 - accuracy: 0.9556 - val_loss: 0.1450 - val_accuracy: 0.9487\n",
            "Epoch 13/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.1442 - accuracy: 0.9472 - val_loss: 0.1505 - val_accuracy: 0.9444\n",
            "Epoch 14/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.1255 - accuracy: 0.9552 - val_loss: 0.1151 - val_accuracy: 0.9571\n",
            "Epoch 15/100\n",
            "176/176 [==============================] - 13s 71ms/step - loss: 0.1216 - accuracy: 0.9557 - val_loss: 0.1090 - val_accuracy: 0.9633\n",
            "Epoch 16/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.1155 - accuracy: 0.9596 - val_loss: 0.1013 - val_accuracy: 0.9639\n",
            "Epoch 17/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.1053 - accuracy: 0.9626 - val_loss: 0.0984 - val_accuracy: 0.9656\n",
            "Epoch 18/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.1087 - accuracy: 0.9612 - val_loss: 0.1163 - val_accuracy: 0.9619\n",
            "Epoch 19/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.1149 - accuracy: 0.9584 - val_loss: 0.1520 - val_accuracy: 0.9481\n",
            "Epoch 20/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.1104 - accuracy: 0.9592 - val_loss: 0.1053 - val_accuracy: 0.9645\n",
            "Epoch 21/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.0983 - accuracy: 0.9652 - val_loss: 0.1106 - val_accuracy: 0.9599\n",
            "Epoch 22/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0945 - accuracy: 0.9661 - val_loss: 0.2265 - val_accuracy: 0.9171\n",
            "Epoch 23/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.1089 - accuracy: 0.9608 - val_loss: 0.0967 - val_accuracy: 0.9669\n",
            "Epoch 24/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0897 - accuracy: 0.9672 - val_loss: 0.1274 - val_accuracy: 0.9551\n",
            "Epoch 25/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0920 - accuracy: 0.9671 - val_loss: 0.0894 - val_accuracy: 0.9699\n",
            "Epoch 26/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0926 - accuracy: 0.9673 - val_loss: 0.0959 - val_accuracy: 0.9669\n",
            "Epoch 27/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0885 - accuracy: 0.9684 - val_loss: 0.0889 - val_accuracy: 0.9704\n",
            "Epoch 28/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0845 - accuracy: 0.9701 - val_loss: 0.1204 - val_accuracy: 0.9587\n",
            "Epoch 29/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0800 - accuracy: 0.9718 - val_loss: 0.0871 - val_accuracy: 0.9699\n",
            "Epoch 30/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0782 - accuracy: 0.9722 - val_loss: 0.1188 - val_accuracy: 0.9556\n",
            "Epoch 31/100\n",
            "176/176 [==============================] - 12s 68ms/step - loss: 0.0759 - accuracy: 0.9723 - val_loss: 0.0851 - val_accuracy: 0.9700\n",
            "Epoch 32/100\n",
            "176/176 [==============================] - 12s 68ms/step - loss: 0.0744 - accuracy: 0.9734 - val_loss: 0.0812 - val_accuracy: 0.9719\n",
            "Epoch 33/100\n",
            "176/176 [==============================] - 12s 68ms/step - loss: 0.0796 - accuracy: 0.9709 - val_loss: 0.0812 - val_accuracy: 0.9721\n",
            "Epoch 34/100\n",
            "176/176 [==============================] - 12s 68ms/step - loss: 0.0667 - accuracy: 0.9764 - val_loss: 0.0877 - val_accuracy: 0.9688\n",
            "Epoch 35/100\n",
            "176/176 [==============================] - 12s 67ms/step - loss: 0.0665 - accuracy: 0.9772 - val_loss: 0.1027 - val_accuracy: 0.9644\n",
            "Epoch 36/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0720 - accuracy: 0.9746 - val_loss: 0.0919 - val_accuracy: 0.9683\n",
            "Epoch 37/100\n",
            "176/176 [==============================] - 13s 74ms/step - loss: 0.0594 - accuracy: 0.9794 - val_loss: 0.1016 - val_accuracy: 0.9641\n",
            "Epoch 38/100\n",
            "176/176 [==============================] - 13s 73ms/step - loss: 0.0662 - accuracy: 0.9772 - val_loss: 0.0901 - val_accuracy: 0.9695\n",
            "Epoch 39/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.0631 - accuracy: 0.9779 - val_loss: 0.0669 - val_accuracy: 0.9773\n",
            "Epoch 40/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.0748 - accuracy: 0.9733 - val_loss: 0.0848 - val_accuracy: 0.9712\n",
            "Epoch 41/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.0608 - accuracy: 0.9794 - val_loss: 0.0866 - val_accuracy: 0.9704\n",
            "Epoch 42/100\n",
            "176/176 [==============================] - 13s 72ms/step - loss: 0.0564 - accuracy: 0.9784 - val_loss: 0.0834 - val_accuracy: 0.9713\n",
            "Epoch 43/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.0494 - accuracy: 0.9825 - val_loss: 0.1058 - val_accuracy: 0.9623\n",
            "Epoch 44/100\n",
            "176/176 [==============================] - 13s 71ms/step - loss: 0.0491 - accuracy: 0.9824 - val_loss: 0.0823 - val_accuracy: 0.9695\n",
            "Epoch 45/100\n",
            "176/176 [==============================] - 13s 71ms/step - loss: 0.0544 - accuracy: 0.9807 - val_loss: 0.1382 - val_accuracy: 0.9513\n",
            "Epoch 46/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.0486 - accuracy: 0.9838 - val_loss: 0.0639 - val_accuracy: 0.9773\n",
            "Epoch 47/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.0457 - accuracy: 0.9840 - val_loss: 0.0915 - val_accuracy: 0.9673\n",
            "Epoch 48/100\n",
            "176/176 [==============================] - 13s 71ms/step - loss: 0.0504 - accuracy: 0.9824 - val_loss: 0.0826 - val_accuracy: 0.9735\n",
            "Epoch 49/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.0554 - accuracy: 0.9801 - val_loss: 0.0670 - val_accuracy: 0.9761\n",
            "Epoch 50/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.0431 - accuracy: 0.9843 - val_loss: 0.0708 - val_accuracy: 0.9759\n",
            "Epoch 51/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0426 - accuracy: 0.9844 - val_loss: 0.1033 - val_accuracy: 0.9631\n",
            "Epoch 52/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0464 - accuracy: 0.9833 - val_loss: 0.0742 - val_accuracy: 0.9756\n",
            "Epoch 53/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.0539 - accuracy: 0.9802 - val_loss: 0.0644 - val_accuracy: 0.9792\n",
            "Epoch 54/100\n",
            "176/176 [==============================] - 13s 71ms/step - loss: 0.0384 - accuracy: 0.9861 - val_loss: 0.0883 - val_accuracy: 0.9685\n",
            "Epoch 55/100\n",
            "176/176 [==============================] - 12s 71ms/step - loss: 0.0418 - accuracy: 0.9863 - val_loss: 0.0933 - val_accuracy: 0.9685\n",
            "Epoch 56/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.0398 - accuracy: 0.9849 - val_loss: 0.0819 - val_accuracy: 0.9727\n",
            "Epoch 57/100\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.0364 - accuracy: 0.9868 - val_loss: 0.0683 - val_accuracy: 0.9769\n",
            "Epoch 58/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0410 - accuracy: 0.9839 - val_loss: 0.0924 - val_accuracy: 0.9645\n",
            "Epoch 59/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0438 - accuracy: 0.9847 - val_loss: 0.0876 - val_accuracy: 0.9716\n",
            "Epoch 60/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0342 - accuracy: 0.9878 - val_loss: 0.0874 - val_accuracy: 0.9723\n",
            "Epoch 61/100\n",
            "176/176 [==============================] - 12s 69ms/step - loss: 0.0318 - accuracy: 0.9897 - val_loss: 0.0741 - val_accuracy: 0.9772\n",
            "Epoch 62/100\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9911\n",
            "Reached 99% accuracy so cancelling training!\n",
            "176/176 [==============================] - 12s 70ms/step - loss: 0.0255 - accuracy: 0.9911 - val_loss: 0.0703 - val_accuracy: 0.9771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFXPzrW2VUK7",
        "colab_type": "text"
      },
      "source": [
        "We see that 99% accuracy has been reached, and the training process has been stopped as a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKknwV5IIkRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}