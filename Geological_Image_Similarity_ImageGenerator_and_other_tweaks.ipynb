{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Geological_Image_Similarity_ImageGenerator_and_other_tweaks",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7vZFV9Ump_X",
        "colab_type": "text"
      },
      "source": [
        "# Classifying Geologically Similar Images with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_onlvcHVdhS",
        "colab_type": "text"
      },
      "source": [
        "This notebook explores how the \"Geological similarity\" dataset can be used for a multi-class classification problem. We pull, prepare and build a model to solve the 6-way classification task to different between different kinds of minerals.\n",
        "\n",
        "Specifically, the objective of this notebook is to demonstrate the Image Generator functionality within `tensorflow.keras`, and the benefit of this when dealing with large, clearly labelled data present as image files on disk. We also demonstrate other elements of tweaking deep neural networks, specifically the addition of convolutional and pooling layers.\n",
        "\n",
        "## Data\n",
        "\n",
        "Some notes about the data:\n",
        "1. The images here are 28x28, colour RGB images.\n",
        "2. There are six classes of images: andesite, gneiss, marble, quartzite  rhyolite, and schist\n",
        "\n",
        "## Experiments\n",
        "\n",
        "Some notes about the model and the experiments:\n",
        "1. Tried building a simple CNN with a single `Conv2D` layer, which didn't perform as well.\n",
        "2. Subsequent iterations increased the number of `Conv2D` layers, married to MaxPooling2D layers\n",
        "3. The number of epochs required to train simpler models to reach ~99% accuracy was high, of the order of 100.\n",
        "4. When more complex models were used, these issues were resolved, with 99% accuracy being reached in 60-odd epochs.\n",
        "\n",
        "\n",
        "## Techniques Demonstrated\n",
        "\n",
        "1. Image Generator - a built-in method within Keras that accelerates the process of creating labelled data by just pointing to a file system folder with the label names being folder names. A very handy tool.\n",
        "2. Early stopping using accuracy callbacks, which enables easier retraining of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcdFk8sGGjkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWZMsuORblcI",
        "colab_type": "text"
      },
      "source": [
        "We pull in the geological similarity data in the below cell, and in subsequent cells, store the data in a local folder in the environment/container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QQZyU4GGjkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_url = \"http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "outputId": "4a7c4d7b-a8bb-4af3-d14d-8c220186194b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip \\\n",
        "    -O /tmp/geological_similarity.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-01 16:05:29--  http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\n",
            "Resolving aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)... 52.218.220.43\n",
            "Connecting to aws-proserve-data-science.s3.amazonaws.com (aws-proserve-data-science.s3.amazonaws.com)|52.218.220.43|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35312590 (34M) [application/zip]\n",
            "Saving to: ‘/tmp/geological_similarity.zip’\n",
            "\n",
            "/tmp/geological_sim 100%[===================>]  33.68M  12.0MB/s    in 2.8s    \n",
            "\n",
            "2020-06-01 16:05:32 (12.0 MB/s) - ‘/tmp/geological_similarity.zip’ saved [35312590/35312590]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/geological_similarity.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCyQ3uLOHLw5",
        "colab_type": "code",
        "outputId": "32cc04c1-7ccb-40c5-97eb-a9a129e6047d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! ls /tmp/geological_similarity/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "andesite  gneiss  marble  quartzite  rhyolite  schist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvWIjCH-buxM",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Model\n",
        "\n",
        "Here, we've defined a `Sequential` model in Keras of relatively high complexity, compared to the simple models we see for the FMNIST data classification task. There are three `Conv2D` layers, with associated pooling layers. The third set of layers uses a smaller filter size compared the earlier ones.\n",
        "\n",
        "The flattened results are then taken to a DNN, which then outputs to a `Softmax` layer to do the multi-class classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2J9jgiJHuug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bK0uqxWH4lB",
        "colab_type": "code",
        "outputId": "f4954f05-2731-4de6-8b2b-ed875b1bbfc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 256)       7168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 128)       295040    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 64)          32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 343,462\n",
            "Trainable params: 343,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aURY4pqXIJca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "'''\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='RMSprop(lr=0.001)',\n",
        "              metrics=['accuracy'])\n",
        "'''\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpAFKLhLcMyb",
        "colab_type": "text"
      },
      "source": [
        "## Using Image Data Generators\n",
        "\n",
        "The image data generator class from Keras' preprocessing module has been used to scale and build the `train_generator` instance. This instance of the class takes in the images in the `geological_similarity` folder, and then prepares the training data for *sparse categorical crossentropy* loss. This means that the output will be a tensor where the number of columns in the tensor will be equal to the number of classes in the classification problem statement.\n",
        "\n",
        "The image data generator makes short work of the potentially laborious task of labelling thousands of images, as long as the images are present in different folders.\n",
        "\n",
        "## Training and Test Sets\n",
        "When specifying the data generator, we can set up a `validation_split` ratio within the constructor, which enables the generator to be pointed to the same location for generating distinct training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lf-chLLISYP",
        "colab_type": "code",
        "outputId": "4228d20f-af30-42ad-b5a0-52680ed60a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "image_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                   validation_split = 0.25)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = image_datagen.flow_from_directory(\n",
        "        '/tmp/geological_similarity',  # This is the source directory for training images\n",
        "        target_size=(28, 28),  # All images will be resized to 28x28\n",
        "        batch_size=128,\n",
        "        # Since we use sparse_categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse',\n",
        "        subset = \"training\")\n",
        "\n",
        "\n",
        "test_generator = image_datagen.flow_from_directory(\n",
        "        '/tmp/geological_similarity',  # This is the source directory for training images\n",
        "        target_size=(28, 28),  # All images will be resized to 28x28\n",
        "        batch_size=128,\n",
        "        # Since we use sparse_categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse',\n",
        "        subset = \"validation\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22499 images belonging to 6 classes.\n",
            "Found 7499 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFKxHdKqczMe",
        "colab_type": "text"
      },
      "source": [
        "The `callbackClass()` class here enables us to stop training the neural network when we reach a certain level of accuracy. In this case, we're looking for 99% accuracy on the training data. We don't have a test dataset here, potentially that could be added as well, if required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_n7AyJcJvls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class callbackClass(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('accuracy')>0.99):\n",
        "                print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "accuracy_filter = callbackClass()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaCXef5dBs4",
        "colab_type": "text"
      },
      "source": [
        "In the cell below, we train the model over 100 maximum epochs, with the accuracy filter enabling us to stop early if the required accuracy has been reached. We can specify additional parameters such as the batch size, and if we possess test data, we could use that too to get validation statistics.\n",
        "\n",
        "## Using Validation Data in Model Training\n",
        "\n",
        "In the model.fit() method, we can introduce the validation_data argument and supply `test_generator` to it. Keras has made this process of supplying validation data really simple!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdLv25-0IhcL",
        "colab_type": "code",
        "outputId": "72392acb-5174-4678-9138-f0e49ab1c0a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      batch_size=16,  \n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      callbacks = [accuracy_filter])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 0.6507 - accuracy: 0.7472 - val_loss: 0.3267 - val_accuracy: 0.8768\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.2781 - accuracy: 0.8943 - val_loss: 0.2222 - val_accuracy: 0.9163\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.2415 - accuracy: 0.9076 - val_loss: 0.2114 - val_accuracy: 0.9209\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.2042 - accuracy: 0.9243 - val_loss: 0.1795 - val_accuracy: 0.9389\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1909 - accuracy: 0.9300 - val_loss: 0.1818 - val_accuracy: 0.9380\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1963 - accuracy: 0.9270 - val_loss: 0.1616 - val_accuracy: 0.9397\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.1625 - accuracy: 0.9428 - val_loss: 0.2074 - val_accuracy: 0.9275\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1760 - accuracy: 0.9353 - val_loss: 0.1845 - val_accuracy: 0.9288\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1584 - accuracy: 0.9415 - val_loss: 0.1690 - val_accuracy: 0.9332\n",
            "Epoch 10/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1534 - accuracy: 0.9450 - val_loss: 0.1362 - val_accuracy: 0.9528\n",
            "Epoch 11/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1494 - accuracy: 0.9463 - val_loss: 0.1493 - val_accuracy: 0.9503\n",
            "Epoch 12/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1243 - accuracy: 0.9553 - val_loss: 0.1376 - val_accuracy: 0.9465\n",
            "Epoch 13/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1479 - accuracy: 0.9464 - val_loss: 0.2024 - val_accuracy: 0.9265\n",
            "Epoch 14/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1396 - accuracy: 0.9498 - val_loss: 0.1473 - val_accuracy: 0.9408\n",
            "Epoch 15/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.1247 - accuracy: 0.9538 - val_loss: 0.1753 - val_accuracy: 0.9387\n",
            "Epoch 16/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1299 - accuracy: 0.9541 - val_loss: 0.1117 - val_accuracy: 0.9629\n",
            "Epoch 17/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.1176 - accuracy: 0.9576 - val_loss: 0.1138 - val_accuracy: 0.9639\n",
            "Epoch 18/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.1186 - accuracy: 0.9555 - val_loss: 0.1037 - val_accuracy: 0.9651\n",
            "Epoch 19/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1036 - accuracy: 0.9639 - val_loss: 0.1125 - val_accuracy: 0.9609\n",
            "Epoch 20/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1060 - accuracy: 0.9625 - val_loss: 0.1056 - val_accuracy: 0.9647\n",
            "Epoch 21/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.1191 - accuracy: 0.9561 - val_loss: 0.1113 - val_accuracy: 0.9613\n",
            "Epoch 22/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0989 - accuracy: 0.9644 - val_loss: 0.1012 - val_accuracy: 0.9676\n",
            "Epoch 23/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0987 - accuracy: 0.9657 - val_loss: 0.0907 - val_accuracy: 0.9695\n",
            "Epoch 24/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0956 - accuracy: 0.9661 - val_loss: 0.1081 - val_accuracy: 0.9644\n",
            "Epoch 25/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0920 - accuracy: 0.9670 - val_loss: 0.0935 - val_accuracy: 0.9679\n",
            "Epoch 26/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0973 - accuracy: 0.9653 - val_loss: 0.0958 - val_accuracy: 0.9676\n",
            "Epoch 27/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0946 - accuracy: 0.9656 - val_loss: 0.0905 - val_accuracy: 0.9700\n",
            "Epoch 28/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0908 - accuracy: 0.9675 - val_loss: 0.0949 - val_accuracy: 0.9669\n",
            "Epoch 29/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0835 - accuracy: 0.9702 - val_loss: 0.0833 - val_accuracy: 0.9721\n",
            "Epoch 30/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0827 - accuracy: 0.9708 - val_loss: 0.0847 - val_accuracy: 0.9729\n",
            "Epoch 31/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0823 - accuracy: 0.9702 - val_loss: 0.0827 - val_accuracy: 0.9732\n",
            "Epoch 32/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0788 - accuracy: 0.9711 - val_loss: 0.0898 - val_accuracy: 0.9691\n",
            "Epoch 33/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0789 - accuracy: 0.9715 - val_loss: 0.1047 - val_accuracy: 0.9644\n",
            "Epoch 34/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0800 - accuracy: 0.9706 - val_loss: 0.0945 - val_accuracy: 0.9663\n",
            "Epoch 35/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0834 - accuracy: 0.9692 - val_loss: 0.1565 - val_accuracy: 0.9411\n",
            "Epoch 36/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0725 - accuracy: 0.9750 - val_loss: 0.1016 - val_accuracy: 0.9653\n",
            "Epoch 37/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0843 - accuracy: 0.9696 - val_loss: 0.1194 - val_accuracy: 0.9549\n",
            "Epoch 38/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0751 - accuracy: 0.9729 - val_loss: 0.0827 - val_accuracy: 0.9713\n",
            "Epoch 39/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0657 - accuracy: 0.9765 - val_loss: 0.0949 - val_accuracy: 0.9660\n",
            "Epoch 40/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0629 - accuracy: 0.9764 - val_loss: 0.0874 - val_accuracy: 0.9713\n",
            "Epoch 41/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0595 - accuracy: 0.9774 - val_loss: 0.0856 - val_accuracy: 0.9712\n",
            "Epoch 42/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0635 - accuracy: 0.9767 - val_loss: 0.0677 - val_accuracy: 0.9780\n",
            "Epoch 43/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0627 - accuracy: 0.9769 - val_loss: 0.0754 - val_accuracy: 0.9737\n",
            "Epoch 44/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0643 - accuracy: 0.9762 - val_loss: 0.0866 - val_accuracy: 0.9688\n",
            "Epoch 45/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0586 - accuracy: 0.9793 - val_loss: 0.1380 - val_accuracy: 0.9553\n",
            "Epoch 46/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0653 - accuracy: 0.9747 - val_loss: 0.0787 - val_accuracy: 0.9744\n",
            "Epoch 47/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0539 - accuracy: 0.9805 - val_loss: 0.0877 - val_accuracy: 0.9703\n",
            "Epoch 48/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0549 - accuracy: 0.9793 - val_loss: 0.0623 - val_accuracy: 0.9796\n",
            "Epoch 49/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0449 - accuracy: 0.9832 - val_loss: 0.1059 - val_accuracy: 0.9635\n",
            "Epoch 50/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0516 - accuracy: 0.9808 - val_loss: 0.1190 - val_accuracy: 0.9541\n",
            "Epoch 51/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0484 - accuracy: 0.9826 - val_loss: 0.0873 - val_accuracy: 0.9719\n",
            "Epoch 52/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0614 - accuracy: 0.9772 - val_loss: 0.0828 - val_accuracy: 0.9721\n",
            "Epoch 53/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0428 - accuracy: 0.9844 - val_loss: 0.0728 - val_accuracy: 0.9769\n",
            "Epoch 54/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0501 - accuracy: 0.9819 - val_loss: 0.1429 - val_accuracy: 0.9572\n",
            "Epoch 55/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0538 - accuracy: 0.9803 - val_loss: 0.0730 - val_accuracy: 0.9748\n",
            "Epoch 56/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0535 - accuracy: 0.9800 - val_loss: 0.0888 - val_accuracy: 0.9692\n",
            "Epoch 57/100\n",
            "176/176 [==============================] - 11s 62ms/step - loss: 0.0502 - accuracy: 0.9809 - val_loss: 0.0655 - val_accuracy: 0.9783\n",
            "Epoch 58/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0358 - accuracy: 0.9877 - val_loss: 0.1106 - val_accuracy: 0.9653\n",
            "Epoch 59/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0407 - accuracy: 0.9852 - val_loss: 0.0774 - val_accuracy: 0.9764\n",
            "Epoch 60/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0597 - accuracy: 0.9788 - val_loss: 0.0666 - val_accuracy: 0.9761\n",
            "Epoch 61/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0374 - accuracy: 0.9874 - val_loss: 0.0773 - val_accuracy: 0.9757\n",
            "Epoch 62/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0353 - accuracy: 0.9871 - val_loss: 0.0686 - val_accuracy: 0.9788\n",
            "Epoch 63/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0331 - accuracy: 0.9878 - val_loss: 0.1127 - val_accuracy: 0.9647\n",
            "Epoch 64/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0369 - accuracy: 0.9867 - val_loss: 0.0654 - val_accuracy: 0.9788\n",
            "Epoch 65/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0291 - accuracy: 0.9890 - val_loss: 0.0650 - val_accuracy: 0.9808\n",
            "Epoch 66/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0380 - accuracy: 0.9854 - val_loss: 0.0662 - val_accuracy: 0.9788\n",
            "Epoch 67/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0372 - accuracy: 0.9869 - val_loss: 0.0768 - val_accuracy: 0.9732\n",
            "Epoch 68/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0308 - accuracy: 0.9894 - val_loss: 0.0882 - val_accuracy: 0.9748\n",
            "Epoch 69/100\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0554 - accuracy: 0.9801 - val_loss: 0.0825 - val_accuracy: 0.9709\n",
            "Epoch 70/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0357 - accuracy: 0.9866 - val_loss: 0.1031 - val_accuracy: 0.9656\n",
            "Epoch 71/100\n",
            "176/176 [==============================] - 11s 61ms/step - loss: 0.0305 - accuracy: 0.9885 - val_loss: 0.0726 - val_accuracy: 0.9761\n",
            "Epoch 72/100\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9903\n",
            "Reached 99% accuracy so cancelling training!\n",
            "176/176 [==============================] - 11s 60ms/step - loss: 0.0263 - accuracy: 0.9903 - val_loss: 0.0924 - val_accuracy: 0.9684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFXPzrW2VUK7",
        "colab_type": "text"
      },
      "source": [
        "We see that 99% accuracy has been reached, and the training process has been stopped as a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKknwV5IIkRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}